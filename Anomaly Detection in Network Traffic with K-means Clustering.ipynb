{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from collections import OrderedDict\n",
    "from numpy import array\n",
    "from math import sqrt\n",
    "from pyspark.mllib.clustering import KMeans\n",
    "from pyspark.mllib.feature import StandardScaler\n",
    "from collections import OrderedDict\n",
    "sc = SparkContext(\"local[2]\", \"First Spark App\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=sc.textFile(\"kddcup.data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting all different labels\n",
      "smurf. 2807886\n",
      "neptune. 1072017\n",
      "normal. 972781\n",
      "satan. 15892\n",
      "ipsweep. 12481\n",
      "portsweep. 10413\n",
      "nmap. 2316\n",
      "back. 2203\n",
      "warezclient. 1020\n",
      "teardrop. 979\n",
      "pod. 264\n",
      "guess_passwd. 53\n",
      "buffer_overflow. 30\n",
      "land. 21\n",
      "warezmaster. 20\n",
      "imap. 12\n",
      "rootkit. 10\n",
      "loadmodule. 9\n",
      "ftp_write. 8\n",
      "multihop. 7\n",
      "phf. 4\n",
      "perl. 3\n",
      "spy. 2\n"
     ]
    }
   ],
   "source": [
    "print (\"Counting all different labels\")\n",
    "labels = data.map(lambda line: line.strip().split(\",\")[-1])\n",
    "label_counts = labels.countByValue()\n",
    "sorted_labels = OrderedDict(sorted(label_counts.items(), key=lambda t: t[1], reverse=True))\n",
    "for label, count in sorted_labels.items():\n",
    "    print (label, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_interaction(line):\n",
    "    \"\"\"\n",
    "    Parses a network data interaction.\n",
    "    \"\"\"\n",
    "    line_split = line.split(\",\")\n",
    "    clean_line_split = [line_split[0]]+line_split[4:-1]\n",
    "    return (line_split[-1], array([float(x) for x in clean_line_split]))\n",
    "\n",
    "\n",
    "def distance(a, b):\n",
    "    \"\"\"\n",
    "    Calculates the euclidean distance between two numeric RDDs\n",
    "    \"\"\"\n",
    "    return sqrt(\n",
    "        a.zip(b)\n",
    "        .map(lambda x: (x[0]-x[1]))\n",
    "        .map(lambda x: x*x)\n",
    "        .reduce(lambda a,b: a+b)\n",
    "        )\n",
    "\n",
    "def dist_to_centroid(datum, clusters):\n",
    "    \"\"\"\n",
    "    Determines the distance of a point to its cluster centroid\n",
    "    \"\"\"\n",
    "    cluster = clusters.predict(datum)\n",
    "    centroid = clusters.centers[cluster]\n",
    "    return sqrt(sum([x**2 for x in (centroid - datum)]))\n",
    "\n",
    "def clustering_score(data, k):\n",
    "    clusters = KMeans.train(data, k, maxIterations=10, runs=5, initializationMode=\"random\")\n",
    "    result = (k, clusters, data.map(lambda datum: dist_to_centroid(datum, clusters)).mean())\n",
    "    print (\"Clustering score for k=%(k)d is %(score)f\" % {\"k\": k, \"score\": result[2]})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing dataset...\n"
     ]
    }
   ],
   "source": [
    "print (\"Parsing dataset...\")\n",
    "parsed_data = data.map(parse_interaction)\n",
    "parsed_data_values = parsed_data.values().cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully imported Spark Modules\n",
      "Standardizing data...\n"
     ]
    }
   ],
   "source": [
    "print (\"Successfully imported Spark Modules\")\n",
    "print (\"Standardizing data...\")\n",
    "standardizer = StandardScaler(True, True)\n",
    "standardizer_model = standardizer.fit(parsed_data_values)\n",
    "standardized_data_values = standardizer_model.transform(parsed_data_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating total in within cluster distance for different k values (10 to 15):\n"
     ]
    }
   ],
   "source": [
    "# Evaluate values of k from 10 to 15\n",
    "max_k=15\n",
    "print (\"Calculating total in within cluster distance for different k values (10 to %(max_k)d):\" % {\"max_k\": max_k})\n",
    "scores = map(lambda k: clustering_score(standardized_data_values, k), range(10,max_k+1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashokvardhan/anaconda/lib/python3.6/site-packages/pyspark/mllib/clustering.py:347: UserWarning: The param `runs` has no effect since Spark 2.0.0.\n",
      "  warnings.warn(\"The param `runs` has no effect since Spark 2.0.0.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering score for k=10 is 1.035337\n",
      "Clustering score for k=11 is 1.062859\n",
      "Clustering score for k=12 is 0.846474\n",
      "Clustering score for k=13 is 0.938197\n",
      "Clustering score for k=14 is 0.775179\n",
      "Clustering score for k=15 is 0.877925\n",
      "Best k value is 14\n"
     ]
    }
   ],
   "source": [
    "# Obtain min score k\n",
    "min_k = min(scores, key=lambda x: x[2])[0]\n",
    "print (\"Best k value is %(best_k)d\" % {\"best_k\": min_k})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining clustering result sample for k=14...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashokvardhan/anaconda/lib/python3.6/site-packages/pyspark/mllib/clustering.py:347: UserWarning: The param `runs` has no effect since Spark 2.0.0.\n",
      "  warnings.warn(\"The param `runs` has no effect since Spark 2.0.0.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering score for k=14 is 0.883399\n"
     ]
    }
   ],
   "source": [
    "scores = map(lambda k: clustering_score(standardized_data_values, k), range(min_k,min_k+1))\n",
    "print (\"Obtaining clustering result sample for k=%(min_k)d...\" % {\"min_k\": min_k})\n",
    "best_model = min(scores, key=lambda x: x[2])[1]\n",
    "cluster_assignments_sample = standardized_data_values.map(lambda datum: str(best_model.predict(datum))+\",\"+\",\".join(map(str,datum))).sample(False,0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[854] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_assignments_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving sample to file...\n"
     ]
    }
   ],
   "source": [
    "# Save assignment sample to file\n",
    "print(\"Saving sample to file...\")\n",
    "cluster_assignments_sample.saveAsTextFile(\"sample_standardized\")\n",
    "print(\"DONE!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
